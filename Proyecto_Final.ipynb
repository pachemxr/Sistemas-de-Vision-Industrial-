{
 "cells": [
  {
   "cell_type": "raw",
   "id": "eec360cf-0e31-4b99-87cd-c2af635499e9",
   "metadata": {},
   "source": [
    "Introducción\n",
    "El presente proyecto tiene como objetivo el desarrollo de un sistema de visión industrial utilizando tecnologías como Python, OpenCV y Arduino. Este sistema es capaz de identificar gestos de la mano en tiempo real y enviar comandos a través de una conexión serial para realizar acciones específicas. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac74319d-1a4d-4d22-9e6b-063317f0a339",
   "metadata": {},
   "source": [
    "Metodología\n",
    "El proyecto va a combinar elementos de visión por computadora y comunicación hardware-software para procesar imágenes capturadas en tiempo real y generar acciones específicas. La implementación comienza con la configuración de la captura de video y la segmentación de la región de interés donde se espera detectar la mano. Posteriormente, se aplican técnicas de procesamiento de imágenes como la calibración del fondo y la detección de contornos. Los datos geométricos de la mano, como su posición y la cantidad de dedos visibles, son procesados para identificar gestos que posteriormente se transmiten al Arduino mediante comunicación serial. Cada uno de estos pasos fue cuidadosamente diseñado y probado para garantizar precisión y confiabilidad en las detecciones."
   ]
  },
  {
   "cell_type": "raw",
   "id": "54388f92-8c2e-4334-a3bd-dbd465236481",
   "metadata": {},
   "source": [
    "A continuación, presentamos el código desarrollado para la detección de gestos de la mano, específicamente los gestos que representan 0, 1 y 2 dedos visibles. Este código utiliza técnicas de procesamiento de imágenes mediante la biblioteca OpenCV, incluyendo la segmentación de la imagen en escala de grises para facilitar la detección. La segmentación permite diferenciar la mano del fondo, y a través del análisis de contornos y convexidades se identifican las características necesarias para determinar el número de dedos presentes."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b83b2fb5-1506-4843-96ed-c1bffadfa046",
   "metadata": {},
   "source": [
    "Importar bibliotecas y definir variables iniciales\n",
    "En este primer bloque inicializamos las bibliotecas necesarias y configuramos las variables globales que controlan la ejecución del programa, incluyendo la gestión del fondo y las dimensiones de la ventana de visualización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09a63820-8d82-439a-9209-661253bf1303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "# Mantener el background frame para quitarlo posteriormente\n",
    "background = None\n",
    "# Guarda los datos de la mano para que todos sus detalles estén en un solo lugar.\n",
    "hand = None\n",
    "# Variables para contar cuántos fotogramas han pasado y para establecer el tamaño de la ventana.\n",
    "frames_elapsed = 0\n",
    "FRAME_HEIGHT = 300\n",
    "FRAME_WIDTH = 400\n",
    "# Prueba a editarlas si tu programa tiene problemas para reconocer tu tono de piel.\n",
    "CALIBRATION_TIME = 80\n",
    "BG_WEIGHT = 0.1\n",
    "OBJ_THRESHOLD = 30"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1d673b9-ad71-4749-a6f1-45158b69e014",
   "metadata": {},
   "source": [
    "Clase HandData\n",
    "Esta clase HandData almacena detalles sobre la posición y movimiento de la mano dentro del área de la cámara. update modifica la posición registrada de la mano basándose en los nuevos datos detectados, y check_for_waving verifica si hay un movimiento significativo de la mano para indicar un gesto de movimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f369b81c-dff7-490f-91fd-499190c95111",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandData:\n",
    "    # Inicialización de variables para almacenar la información geométrica y el estado de la mano\n",
    "    top = (0,0)\n",
    "    bottom = (0,0)\n",
    "    left = (0,0)\n",
    "    right = (0,0)\n",
    "    centerX = 0  # Posición central de la mano en el eje X\n",
    "    prevCenterX = 0  # Posición X anterior para detectar movimientos\n",
    "    isInFrame = False  # Indica si la mano está en el marco\n",
    "    isWaving = False  # Indica si la mano está haciendo un gesto de movimiento\n",
    "    fingers = None  # Almacena el número de dedos detectados\n",
    "    gestureList = []  # Lista para almacenar los gestos detectados temporalmente\n",
    "\n",
    "    def __init__(self, top, bottom, left, right, centerX):\n",
    "        # Constructor que inicializa las propiedades de la mano con valores específicos\n",
    "        self.top = top\n",
    "        self.bottom = bottom\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.centerX = centerX\n",
    "        self.prevCenterX = 0  # Se inicia en cero para futuras comparaciones\n",
    "        self.isInFrame = False  # Se asume que la mano no está en el marco inicialmente\n",
    "        self.isWaving = False  # Se asume que no hay movimiento inicialmente\n",
    "\n",
    "    def update(self, top, bottom, left, right):\n",
    "        # Método para actualizar la posición de la mano en el marco\n",
    "        self.top = top\n",
    "        self.bottom = bottom\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def check_for_waving(self, centerX):\n",
    "        # Método para verificar si la mano está haciendo un gesto de movimiento (onda)\n",
    "        self.prevCenterX = self.centerX  # Guarda la posición X actual como previa\n",
    "        self.centerX = centerX  # Actualiza la posición central X con el nuevo valor\n",
    "        # Comprueba si el movimiento entre las dos posiciones X es mayor que 3\n",
    "        if abs(self.centerX - self.prevCenterX) > 3:\n",
    "            self.isWaving = True  # Si es así, se considera que la mano está ondeando\n",
    "        else:\n",
    "            self.isWaving = False  # De lo contrario, no hay movimiento significativo"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebf724fd-159c-43e6-8f09-107e4ddbb213",
   "metadata": {},
   "source": [
    "Funciones de procesamiento de imágenes y gestos\n",
    "Estas funciones manejan la extracción y procesamiento de la región de interés, la calibración del fondo, la segmentación de la mano, y la actualización del frame con información relevante sobre el estado de la mano o el proceso. La función segment es crucial para diferenciar la mano del fondo y detectar los contornos de la mano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cd53b05-8f2a-4eca-b51a-662814077b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_on_image(frame):\n",
    "    \"\"\"\n",
    "    Actualiza la imagen capturada con información textual sobre el estado de detección de la mano\n",
    "    y dibuja un rectángulo alrededor de la región de interés.\n",
    "\n",
    "    Args:\n",
    "    frame: El frame actual obtenido de la cámara.\n",
    "\n",
    "    La función verifica el estado de la detección:\n",
    "    - Si está en periodo de calibración, muestra 'Calibrando...'.\n",
    "    - Si no se detecta la mano, muestra 'Mano no detectada'.\n",
    "    - Si se detecta la mano y está realizando un movimiento, muestra 'Moviendo'.\n",
    "    - Muestra el número de dedos detectados si la mano está estática.\n",
    "    \"\"\"\n",
    "    # Inicializa el texto a mostrar en función del estado de la mano.\n",
    "    text = \"Buscando...\"\n",
    "    if frames_elapsed < CALIBRATION_TIME:\n",
    "        text = \"Calibrando...\"\n",
    "    elif hand is None or not hand.isInFrame:\n",
    "        text = \"Mano no detectada\"\n",
    "    else:\n",
    "        if hand.isWaving:\n",
    "            text = \"Moviendo\"\n",
    "        elif hand.fingers == 0:\n",
    "            text = \"Cero\"\n",
    "        elif hand.fingers == 1:\n",
    "            text = \"Uno\"\n",
    "        elif hand.fingers == 2:\n",
    "            text = \"Dos\"\n",
    "\n",
    "    # Dibuja el texto en el frame dos veces para crear un efecto de borde que mejore la legibilidad.\n",
    "    cv2.putText(frame, text, (10, 20), cv2.FONT_HERSHEY_COMPLEX, 0.4, (0, 0, 0), 2)\n",
    "    cv2.putText(frame, text, (10, 20), cv2.FONT_HERSHEY_COMPLEX, 0.4, (255, 255, 255), 1)\n",
    "\n",
    "    # Dibuja un rectángulo alrededor de la región de interés para indicar dónde debe colocarse la mano.\n",
    "    cv2.rectangle(frame, (region_left, region_top), (region_right, region_bottom), (255, 255, 255), 2)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9571a643-1908-438a-8bfd-c533e195949c",
   "metadata": {},
   "source": [
    "Función get_region()\n",
    "Esta función es responsable de preparar la región de interés (ROI) para la detección de bordes y la posterior segmentación de la mano. Extrae la ROI del frame, la convierte a escala de grises y aplica un filtro Gaussiano para reducir el ruido, lo cual facilita la detección de bordes y contornos de la mano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c8c495d-f2ee-4d4b-8d1d-e573be8e7980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_region(frame):\n",
    "    # Esta línea extrae una subsección del marco original basada en las coordenadas predefinidas.\n",
    "    # La región extraída es donde se espera encontrar o analizar la mano o cualquier objeto de interés.\n",
    "    region = frame[region_top:region_bottom, region_left:region_right]\n",
    "\n",
    "    # Convertir la región de interés a escala de grises facilita el procesamiento de imagen siguiente,\n",
    "    # porque reduce la complejidad de la imagen al eliminar la información de color,\n",
    "    # dejando solo la intensidad de los píxeles que es más útil para la detección de bordes.\n",
    "    region = cv2.cvtColor(region, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Aplicar un filtro gaussiano ayuda a suavizar la imagen, lo que reduce el ruido y las variaciones de intensidad.\n",
    "    # Esto es particularmente útil para preparar la imagen para procesos de detección de bordes y contornos,\n",
    "    # ya que evita que se detecten falsos positivos causados por pequeñas imperfecciones o ruido en la imagen.\n",
    "    region = cv2.GaussianBlur(region, (5,5), 0)\n",
    "\n",
    "    # Devuelve la región de interés procesada y lista para la siguiente etapa de procesamiento de imagen.\n",
    "    return region"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c2d00fb-6aea-4ba8-abac-aab466d6c609",
   "metadata": {},
   "source": [
    "Función get_average()\n",
    "Esta función desarrolla un fondo promedio para la diferenciación de imágenes, lo que es crucial para la técnica de sustracción de fondo en la detección de movimiento. Si es la primera vez que se llama, inicializa el fondo con la región actual. Posteriormente, actualiza el fondo utilizando un promedio ponderado para adaptarse a los cambios en el video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d9d5128-e391-405f-80da-fbba1119015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average(region):\n",
    "    \"\"\"\n",
    "    Actualiza el fondo para sustracción de fondo utilizando un promedio ponderado.\n",
    "    Esta técnica ayuda a identificar y segmentar objetos en movimiento en la imagen.\n",
    "\n",
    "    Args:\n",
    "    region (ndarray): La región actual de interés en la imagen capturada de la cámara.\n",
    "\n",
    "    La función verifica si ya se ha establecido un fondo. Si no es así, inicializa el fondo\n",
    "    con la región actual. Si el fondo ya existe, actualiza este fondo con un promedio ponderado\n",
    "    de las imágenes actuales, lo que permite adaptar el fondo a los cambios graduales en la\n",
    "    escena como cambios en la iluminación o en la configuración del entorno.\n",
    "    \"\"\"\n",
    "\n",
    "    # Utilizamos la variable global 'background' para mantener el estado del fondo a través de las llamadas a la función\n",
    "    global background\n",
    "\n",
    "    # Si 'background' no está inicializado, lo establecemos con la región actual\n",
    "    if background is None:\n",
    "        # Copia la región a 'background' y convierte a tipo float para cálculos futuros\n",
    "        background = region.copy().astype(\"float\")\n",
    "        return  # Finaliza la función después de inicializar el fondo\n",
    "\n",
    "    # Si el fondo ya está inicializado, actualiza el fondo con un promedio ponderado\n",
    "    # Esto permite que el fondo se ajuste a cambios en la escena\n",
    "    cv2.accumulateWeighted(region, background, BG_WEIGHT)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e095bff-ec1f-42cb-a0aa-6581fad89fdc",
   "metadata": {},
   "source": [
    "Función segment()\n",
    "Utiliza una diferenciación de imágenes para identificar la mano separándola del fondo establecido. Esta seria clave para detectar la mano en el área de interés al aplicar un umbral que crea una imagen binaria donde el primer plano (mano) es claramente visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f5f2384-b9bb-4afd-8c1c-dde48f5986bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(region):\n",
    "    \"\"\"\n",
    "    Segmenta la mano del fondo en una región de interés (ROI) mediante diferenciación y umbralización.\n",
    "    Devuelve la región segmentada y el contorno de la mano si se detecta alguna.\n",
    "\n",
    "    Args:\n",
    "    region (array): La ROI del frame actual en escala de grises.\n",
    "    \"\"\"\n",
    "    global hand  # Referencia al objeto global que representa la mano detectada.\n",
    "\n",
    "    # Calcula la diferencia absoluta entre el fondo promediado y el frame actual para resaltar cambios.\n",
    "    diff = cv2.absdiff(background.astype(np.uint8), region)\n",
    "\n",
    "    # Aplica un umbral binario para convertir la diferencia en una imagen binaria donde los valores altos indican movimiento.\n",
    "    thresholded_region = cv2.threshold(diff, OBJ_THRESHOLD, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # Encuentra los contornos en la imagen umbralizada. Los contornos son útiles para identificar formas como la mano.\n",
    "    contours, _ = cv2.findContours(thresholded_region.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Si no se encuentran contornos, se considera que no hay mano en la región.\n",
    "    if len(contours) == 0:\n",
    "        if hand is not None:\n",
    "            hand.isInFrame = False  # Actualiza el estado de la mano a no detectada.\n",
    "        return  # Sale de la función sin retornar objetos.\n",
    "\n",
    "    # Si se encuentran contornos, se selecciona el mayor contorno que probablemente sea la mano.\n",
    "    else:\n",
    "        if hand is not None:\n",
    "            hand.isInFrame = True  # Actualiza el estado de la mano a detectada.\n",
    "        segmented_region = max(contours, key=cv2.contourArea)  # Elige el contorno de área máxima.\n",
    "\n",
    "        # Devuelve la imagen umbralizada junto con el contorno más grande encontrado.\n",
    "        return (thresholded_region, segmented_region)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1066f5a2-d08e-44f4-b396-aa43309a18e6",
   "metadata": {},
   "source": [
    "Función get_hand_data()\n",
    "Analiza la imagen segmentada de la mano para detectar características como las extremidades utilizando el algoritmo convexHull. Esta función identifica puntos críticos como el top, bottom, left, y right de la mano, y calcula el centro para monitorizar movimientos como el ondeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "898ae43d-5c1c-45bc-be4b-258c33156c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hand_data(thresholded_image, segmented_image):\n",
    "    global hand  # Utiliza la variable 'hand' definida globalmente para almacenar datos de la mano\n",
    "\n",
    "    # Utiliza Convex Hull para encontrar el contorno que conecta todos los puntos extremos de la mano\n",
    "    convexHull = cv2.convexHull(segmented_image)\n",
    "\n",
    "    # Extrae las coordenadas extremas del convex hull para definir los límites de la mano\n",
    "    top = tuple(convexHull[convexHull[:, :, 1].argmin()][0])\n",
    "    bottom = tuple(convexHull[convexHull[:, :, 1].argmax()][0])\n",
    "    left = tuple(convexHull[convexHull[:, :, 0].argmin()][0])\n",
    "    right = tuple(convexHull[convexHull[:, :, 0].argmax()][0])\n",
    "\n",
    "    # Calcula el centro de la mano para ayudar en la detección de movimientos y la localización de dedos\n",
    "    centerX = int((left[0] + right[0]) / 2)\n",
    "\n",
    "    # Si 'hand' no está inicializada, crea una nueva instancia de HandData con los valores obtenidos\n",
    "    if hand == None:\n",
    "        hand = HandData(top, bottom, left, right, centerX)\n",
    "    else:\n",
    "        # Si ya está inicializada, actualiza la información de la mano\n",
    "        hand.update(top, bottom, left, right)\n",
    "\n",
    "    # Verifica si la mano está ondeando, una vez cada 6 fotogramas para reducir la sensibilidad\n",
    "    if frames_elapsed % 6 == 0:\n",
    "        hand.check_for_waving(centerX)\n",
    "\n",
    "    # Añade el recuento de dedos a una lista de gestos para promediar el resultado\n",
    "    hand.gestureList.append(count_fingers(thresholded_image))\n",
    "    # Determina el número de dedos más comúnmente identificado cada 12 fotogramas\n",
    "    if frames_elapsed % 12 == 0:\n",
    "        hand.fingers = most_frequent(hand.gestureList)\n",
    "        hand.gestureList.clear()  # Limpia la lista de gestos para el próximo ciclo de conteo"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd68061f-f01a-4588-af98-1c4a3b2aab70",
   "metadata": {},
   "source": [
    "Función count_fingers()\n",
    "Esta función calcula el número de dedos visibles en una imagen binarizada (umbralizada) de una mano. Utiliza métodos de procesamiento de imágenes para detectar cambios de intensidad que corresponden a los bordes de los dedos. La técnica específica incluye trazar una línea horizontal a través de la mano y luego contar los cruces de esta línea con los contornos de los dedos, que se asumen como cambios en la intensidad de la imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28b17f85-7f12-4992-a1ca-231907aec273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_fingers(thresholded_image):\n",
    "    # Determinar la altura en la imagen donde se contará los dedos, basado en la posición superior e inferior de la mano detectada.\n",
    "    line_height = int(hand.top[1] + (0.2 * (hand.bottom[1] - hand.top[1])))\n",
    "\n",
    "    # Crear una imagen binaria donde se trazará la línea para contar los dedos.\n",
    "    line = np.zeros(thresholded_image.shape[:2], dtype=int)\n",
    "\n",
    "    # Dibujar una línea horizontal a la altura calculada que abarque todo el ancho de la imagen.\n",
    "    cv2.line(line, (thresholded_image.shape[1], line_height), (0, line_height), 255, 1)\n",
    "\n",
    "    # Realizar una operación AND a nivel de bit para resaltar donde la línea intersecta con los dedos en la imagen umbralizada.\n",
    "    line = cv2.bitwise_and(thresholded_image, thresholded_image, mask=line.astype(np.uint8))\n",
    "\n",
    "    # Detectar los contornos en la línea donde se intersectan los dedos, interpretando cada sección continua como un dedo.\n",
    "    contours, _ = cv2.findContours(line.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    # Inicializar contador de dedos.\n",
    "    fingers = 0\n",
    "\n",
    "    # Contar los dedos asegurando que cada contorno detectado no sea demasiado ancho para ser un dedo.\n",
    "    for curr in contours:\n",
    "        width = len(curr)\n",
    "        # Confirmar que el contorno es lo suficientemente delgado para ser considerado un dedo.\n",
    "        if width < 3 * abs(hand.right[0] - hand.left[0]) / 4 and width > 5:\n",
    "            fingers += 1\n",
    "\n",
    "    # Devolver el número total de dedos detectados.\n",
    "    return fingers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4525f67d-886a-4c24-97f3-2e07d68823f3",
   "metadata": {},
   "source": [
    "La función most_frequent es un algoritmo diseñado para identificar el elemento que más frecuentemente aparece en una lista. Este tipo de función es comúnmente usada en aplicaciones de procesamiento de datos donde se requiere determinar el modo o valor más común dentro de un conjunto de datos. Es particularmente útil en contextos de visión por computadora y aprendizaje automático para consolidar resultados repetitivos, como reconocer el gesto más frecuente en un intervalo de tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5168b09c-8e31-41bb-aac7-080a3cd17354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(input_list):\n",
    "    # Diccionario para contar la frecuencia de cada elemento en la lista\n",
    "    freq_dict = {}\n",
    "    count = 0  # Variable para almacenar la frecuencia máxima actual\n",
    "    most_freq = 0  # Variable para almacenar el elemento más frecuente\n",
    "\n",
    "    # Itera sobre la lista en orden inverso para priorizar el último valor más frecuente en caso de empates\n",
    "    for item in reversed(input_list):\n",
    "        # Incrementa el conteo del elemento en el diccionario\n",
    "        freq_dict[item] = freq_dict.get(item, 0) + 1\n",
    "\n",
    "        # Si el conteo del elemento actual es mayor o igual al mayor conteo registrado\n",
    "        if freq_dict[item] >= count:\n",
    "            count = freq_dict[item]  # Actualiza el mayor conteo\n",
    "            most_freq = item  # Actualiza el elemento más frecuente\n",
    "\n",
    "    # Devuelve el elemento que más frecuentemente aparece en la lista\n",
    "    return most_freq\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70f9874d-d359-46a2-8d09-f7062513ec35",
   "metadata": {},
   "source": [
    "Configuración inicial y bucle de captura de vídeo\n",
    "Este bloque de código configura y procesa el vídeo en tiempo real para detectar y analizar la mano en una región de interés definida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f50d2921-34d5-47a7-942d-43d30b81fcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Definición de la región de interés en la parte superior derecha del frame\n",
    "region_top = 0\n",
    "region_bottom = int(2 * FRAME_HEIGHT / 3)\n",
    "region_left = int(FRAME_WIDTH / 2)\n",
    "region_right = FRAME_WIDTH\n",
    "\n",
    "frames_elapsed = 0  # Contador de fotogramas para gestionar la calibración\n",
    "capture = cv2.VideoCapture(0)  # Inicia la captura de vídeo con la cámara predeterminada\n",
    "\n",
    "# Bucle infinito para el procesamiento de cada frame capturado\n",
    "while True:\n",
    "    ret, frame = capture.read()  # Captura un frame de la cámara\n",
    "    if not ret:\n",
    "        break  # Si no hay frame, rompe el bucle\n",
    "\n",
    "    # Redimensiona el frame al tamaño especificado y lo voltea horizontalmente\n",
    "    frame = cv2.resize(frame, (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "    frame = cv2.flip(frame, 1)  # Voltea el frame para simular un efecto espejo\n",
    "\n",
    "    # Obtiene la región de interés preparada para la detección de bordes\n",
    "    region = get_region(frame)\n",
    "\n",
    "    # Proceso de calibración y segmentación\n",
    "    if frames_elapsed < CALIBRATION_TIME:\n",
    "        get_average(region)  # Calcula el promedio del fondo durante el tiempo de calibración\n",
    "    else:\n",
    "        # Segmenta la mano del fondo una vez finalizada la calibración\n",
    "        region_pair = segment(region)\n",
    "        if region_pair is not None:\n",
    "            # Si la segmentación fue exitosa, muestra la región segmentada\n",
    "            thresholded_region, segmented_region = region_pair\n",
    "            cv2.drawContours(region, [segmented_region], -1, (255, 255, 255))\n",
    "            cv2.imshow(\"Segmented Image\", region)  # Muestra la imagen segmentada\n",
    "\n",
    "            # Actualiza los datos de la mano basados en la imagen segmentada\n",
    "            get_hand_data(thresholded_region, segmented_region)\n",
    "\n",
    "    # Actualiza el frame con la información de la mano y muestra el resultado\n",
    "    write_on_image(frame)\n",
    "    cv2.imshow(\"Camera Input\", frame)  # Muestra el frame en la ventana\n",
    "\n",
    "    frames_elapsed += 1  # Incrementa el contador de fotogramas\n",
    "\n",
    "    # Permite salir del bucle con la tecla 'x'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('c'):\n",
    "        break\n",
    "\n",
    "# Limpieza: libera la captura de la cámara y cierra todas las ventanas abiertas\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cfd4ab-9079-4232-8dcd-7a3a5b2db426",
   "metadata": {},
   "source": [
    "Codigo Completo, Funcional para 2 dedos y Moviendo"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7d5b09e-9926-41a1-b837-abae5ac367c0",
   "metadata": {},
   "source": [
    "A continuación, se presenta el código completo del sistema de visión industrial desarrollado, el cual establece una comunicación directa entre Python y Arduino para controlar acciones basadas en gestos de la mano detectados. El sistema utiliza una cámara para capturar imágenes en tiempo real, procesarlas mediante técnicas de visión por computadora como segmentación y análisis de contornos, y enviar comandos al Arduino a través de un puerto serial. Este enfoque permite traducir gestos simples, como mostrar 0, 1 o 2 dedos, en señales que activan o controlan dispositivos físicos, demostrando la integración práctica de software y hardware en aplicaciones industriales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a0939f9c-9cdd-4746-8a98-71378e3fe17e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SerialException",
     "evalue": "WriteFile failed (PermissionError(13, 'El dispositivo no reconoce el comando.', None, 22))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSerialException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 166\u001b[0m\n\u001b[0;32m    164\u001b[0m         thresholded_region, segmented_region \u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m    165\u001b[0m         get_hand_data(thresholded_region, segmented_region)\n\u001b[1;32m--> 166\u001b[0m write_on_image(frame)\n\u001b[0;32m    167\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCamera Input\u001b[39m\u001b[38;5;124m\"\u001b[39m, frame)\n\u001b[0;32m    168\u001b[0m frames_elapsed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[45], line 67\u001b[0m, in \u001b[0;36mwrite_on_image\u001b[1;34m(frame)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hand\u001b[38;5;241m.\u001b[39mfingers \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     66\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCero\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 67\u001b[0m     ser\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hand\u001b[38;5;241m.\u001b[39mfingers \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     69\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUno\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\serial\\serialwin32.py:317\u001b[0m, in \u001b[0;36mSerial.write\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_timeout \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# if blocking (None) or w/ write timeout (>0)\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success \u001b[38;5;129;01mand\u001b[39;00m win32\u001b[38;5;241m.\u001b[39mGetLastError() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (win32\u001b[38;5;241m.\u001b[39mERROR_SUCCESS, win32\u001b[38;5;241m.\u001b[39mERROR_IO_PENDING):\n\u001b[1;32m--> 317\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SerialException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriteFile failed (\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(ctypes\u001b[38;5;241m.\u001b[39mWinError()))\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;66;03m# Wait for the write to complete.\u001b[39;00m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m#~ win32.WaitForSingleObject(self._overlapped_write.hEvent, win32.INFINITE)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     win32\u001b[38;5;241m.\u001b[39mGetOverlappedResult(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_port_handle, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_overlapped_write, ctypes\u001b[38;5;241m.\u001b[39mbyref(n), \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mSerialException\u001b[0m: WriteFile failed (PermissionError(13, 'El dispositivo no reconoce el comando.', None, 22))"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import serial\n",
    "import time\n",
    "\n",
    "# Configuración inicial de la comunicación serial\n",
    "ser = serial.Serial('COM11', 9600, timeout=2)  # Ajusta el nombre del puerto y la tasa de baudios\n",
    "time.sleep(3)  # Espera para que la conexión se establezca\n",
    "\n",
    "# Dimensiones y regiones de la imagen\n",
    "FRAME_HEIGHT = 300\n",
    "FRAME_WIDTH = 400\n",
    "region_top = 0\n",
    "region_bottom = int(2 * FRAME_HEIGHT / 3)\n",
    "region_left = int(FRAME_WIDTH / 2)\n",
    "region_right = FRAME_WIDTH\n",
    "\n",
    "# Configuraciones de procesamiento de imagen\n",
    "background = None\n",
    "hand = None\n",
    "frames_elapsed = 0\n",
    "CALIBRATION_TIME = 80\n",
    "BG_WEIGHT = 0.2\n",
    "OBJ_THRESHOLD = 30\n",
    "\n",
    "class HandData:\n",
    "    # Inicialización de variables para almacenar la información geométrica y el estado de la mano\n",
    "    def __init__(self, top, bottom, left, right, centerX):\n",
    "        self.top = top\n",
    "        self.bottom = bottom\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.centerX = centerX\n",
    "        self.prevCenterX = 0\n",
    "        self.isInFrame = False\n",
    "        self.isWaving = False\n",
    "        self.fingers = None\n",
    "        self.gestureList = []\n",
    "\n",
    "    def update(self, top, bottom, left, right):\n",
    "        self.top = top\n",
    "        self.bottom = bottom\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def check_for_waving(self, centerX):\n",
    "        self.prevCenterX = self.centerX\n",
    "        self.centerX = centerX\n",
    "        if abs(self.centerX - self.prevCenterX) > 3:\n",
    "            self.isWaving = True\n",
    "            ser.write(b'3')\n",
    "        else:\n",
    "            self.isWaving = False\n",
    "\n",
    "def write_on_image(frame):\n",
    "    text = \"Buscando...\"\n",
    "    if frames_elapsed < CALIBRATION_TIME:\n",
    "        text = \"Calibrando...\"\n",
    "    elif not hand or not hand.isInFrame:\n",
    "        text = \"Mano no detectada\"\n",
    "    else:\n",
    "        if hand.isWaving:\n",
    "            text = \"Moviendo\"\n",
    "        elif hand.fingers is not None:\n",
    "            if hand.fingers == 0:\n",
    "                text = \"Cero\"\n",
    "                ser.write(b'0')\n",
    "            elif hand.fingers == 1:\n",
    "                text = \"Uno\"\n",
    "                ser.write(b'1')\n",
    "            elif hand.fingers == 2:\n",
    "                text = \"Dos\"\n",
    "                ser.write(b'2')\n",
    "\n",
    "    cv2.putText(frame, text, (10, 20), cv2.FONT_HERSHEY_COMPLEX, 0.4, (0, 0, 0), 2)\n",
    "    cv2.putText(frame, text, (10, 20), cv2.FONT_HERSHEY_COMPLEX, 0.4, (255, 255, 255), 1)\n",
    "    cv2.rectangle(frame, (region_left, region_top), (region_right, region_bottom), (255, 255, 255), 2)\n",
    "\n",
    "\n",
    "def get_region(frame):\n",
    "    region = frame[region_top:region_bottom, region_left:region_right]\n",
    "    region = cv2.cvtColor(region, cv2.COLOR_BGR2GRAY)\n",
    "    region = cv2.GaussianBlur(region, (5,5), 0)\n",
    "    return region\n",
    "\n",
    "def get_average(region):\n",
    "    global background\n",
    "    if background is None:\n",
    "        background = region.copy().astype(\"float\")\n",
    "    else:\n",
    "        cv2.accumulateWeighted(region, background, BG_WEIGHT)\n",
    "\n",
    "def segment(region):\n",
    "    global hand\n",
    "    diff = cv2.absdiff(cv2.convertScaleAbs(background), region)\n",
    "    thresholded_region = cv2.threshold(diff, OBJ_THRESHOLD, 255, cv2.THRESH_BINARY)[1]\n",
    "    contours, _ = cv2.findContours(thresholded_region.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        if hand is not None:\n",
    "            hand.isInFrame = False\n",
    "        return None\n",
    "    max_contour = max(contours, key=cv2.contourArea)\n",
    "    if hand is not None:\n",
    "        hand.isInFrame = True\n",
    "    return thresholded_region, max_contour\n",
    "\n",
    "def get_hand_data(thresholded_image, segmented_image):\n",
    "    global hand\n",
    "    convexHull = cv2.convexHull(segmented_image)\n",
    "    top = tuple(convexHull[convexHull[:, :, 1].argmin()][0])\n",
    "    bottom = tuple(convexHull[convexHull[:, :, 1].argmax()][0])\n",
    "    left = tuple(convexHull[convexHull[:, :, 0].argmin()][0])\n",
    "    right = tuple(convexHull[convexHull[:, :, 0].argmax()][0])\n",
    "    centerX = int((left[0] + right[0]) / 2)\n",
    "    if hand is None:\n",
    "        hand = HandData(top, bottom, left, right, centerX)\n",
    "    else:\n",
    "        hand.update(top, bottom, left, right)\n",
    "    if frames_elapsed % 6 == 0:\n",
    "        hand.check_for_waving(centerX)\n",
    "    hand.gestureList.append(count_fingers(thresholded_image))\n",
    "    if frames_elapsed % 12 == 0:\n",
    "        hand.fingers = most_frequent(hand.gestureList)\n",
    "        hand.gestureList.clear()\n",
    "\n",
    "def count_fingers(thresholded_image):\n",
    "    line_height = int(hand.top[1] + (0.2 * (hand.bottom[1] - hand.top[1])))\n",
    "    line = np.zeros(thresholded_image.shape[:2], dtype=int)\n",
    "    cv2.line(line, (thresholded_image.shape[1], line_height), (0, line_height), 255, 1)\n",
    "    line = cv2.bitwise_and(thresholded_image, thresholded_image, mask=line.astype(np.uint8))\n",
    "    contours, _ = cv2.findContours(line.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    fingers = 0\n",
    "    for curr in contours:\n",
    "        width = len(curr)\n",
    "        if width < 3 * abs(hand.right[0] - hand.left[0]) / 4 and width > 5:\n",
    "            fingers += 1\n",
    "    return fingers\n",
    "\n",
    "def most_frequent(input_list):\n",
    "    freq_dict = {}\n",
    "    count = 0\n",
    "    most_freq = 0\n",
    "    for item in reversed(input_list):\n",
    "        freq_dict[item] = freq_dict.get(item, 0) + 1\n",
    "        if freq_dict[item] >= count:\n",
    "            count = freq_dict[item]\n",
    "            most_freq = item\n",
    "    return most_freq\n",
    "\n",
    "capture = cv2.VideoCapture(0)  # Asegúrate de que el índice de la cámara es correcto\n",
    "\n",
    "while True:\n",
    "    ret, frame = capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame = cv2.resize(frame, (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    region = get_region(frame)\n",
    "    if frames_elapsed < CALIBRATION_TIME:\n",
    "        get_average(region)\n",
    "    else:\n",
    "        result = segment(region)\n",
    "        if result:\n",
    "            thresholded_region, segmented_region = result\n",
    "            get_hand_data(thresholded_region, segmented_region)\n",
    "    write_on_image(frame)\n",
    "    cv2.imshow(\"Camera Input\", frame)\n",
    "    frames_elapsed += 1\n",
    "    if cv2.waitKey(1) & 0xFF == ord('c'):\n",
    "        break\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "ser.close()  # Cierra el puerto serial al finalizar"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e895c00e-5151-4d6a-a313-47d14c249d96",
   "metadata": {},
   "source": [
    "Resultados \n",
    "El sistema fue capaz de detectar de manera consistente la presencia de una mano en el cuadro de video y reconocer gestos básicos, como mostrar entre 0 y 2 dedos o realizar movimientos de vaivén. Estos gestos fueron asociados con comandos específicos enviados al Arduino, logrando acciones programadas como el encendido y apagado de un LED. El procesamiento en tiempo real se mantuvo dentro de parámetros aceptables, demostrando que el sistema puede operar sin interrupciones significativas y con un rendimiento estable.\n",
    "\n",
    "Discusión\n",
    "La implementación de este sistema permitió explorar la interacción entre software y hardware en un contexto industrial. La segmentación y el procesamiento de imágenes en tiempo real representaron retos clave que fueron superados mediante ajustes iterativos en los algoritmos y parámetros. Además, la comunicación serial con Arduino permitió que los gestos detectados se transformaran en acciones físicas, demostrando el potencial de este tipo de sistemas para aplicaciones industriales. A pesar de los resultados exitosos, se identificaron oportunidades de mejora, como la optimización del procesamiento para aumentar la velocidad y la inclusión de algoritmos más avanzados para reconocer una mayor variedad de gestos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9f4e3b9-4c38-4f1e-b5a1-e34947b12b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "ser.close()  # Cierra el puerto serial al finalizar"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27de54b2-f9e5-4f0c-82e1-2a584817edaf",
   "metadata": {},
   "source": [
    "Conclusión\n",
    "El desarrollo de este sistema de visión industrial nos permitió no solo implementar una solución técnica funcional, sino también explorar a fondo los principios y técnicas que posibilitan la interacción fluida entre software, hardware y datos en tiempo real. Este proyecto fue un ejercicio integral que combinó conocimientos de programación, procesamiento de imágenes, diseño de algoritmos y control de hardware, fortaleciendo nuestra capacidad para abordar problemas técnicos complejos desde una perspectiva multidisciplinaria.\n",
    "\n",
    "En particular, la experiencia de diseñar un sistema que reconoce gestos de la mano y los traduce en comandos físicos nos enseñó la importancia de la precisión y la optimización en sistemas en tiempo real. Aprendimos cómo el uso de bibliotecas avanzadas como OpenCV, junto con la comunicación efectiva a través de puertos seriales, puede dar lugar a soluciones robustas que conectan el mundo digital con el físico. Este tipo de integración es crucial en la industria moderna, donde las tecnologías inteligentes y automatizadas juegan un papel cada vez más relevante.\n",
    "\n",
    "Asimismo, este aprendizaje refuerza nuestra formación como ingenieros mecatrónicos, dotándonos de las herramientas necesarias para enfrentar desafíos tecnológicos de la era digital. Proyectos como este nos preparan para diseñar e implementar soluciones innovadoras en sectores clave como la manufactura, la robótica, y la automatización de procesos industriales. Nos brinda una visión práctica de cómo nuestras habilidades técnicas pueden contribuir al desarrollo de sistemas eficientes, precisos y sostenibles que respondan a las necesidades de un mundo interconectado.\n",
    "\n",
    "En términos de impacto profesional, este proyecto representa un avance significativo en nuestra preparación para contribuir al progreso de la industria. Nos permite comprender mejor cómo los principios fundamentales de nuestra disciplina pueden aplicarse para crear tecnologías de alto impacto y nos inspira a seguir innovando en un entorno de constantes avances tecnológicos. Además, refuerza nuestra habilidad para trabajar con herramientas modernas y adaptarnos a las demandas cambiantes del mercado laboral en la era de la automatización y la inteligencia artificial. Este sistema no solo refleja un logro técnico, sino también un paso importante en nuestro desarrollo como ingenieros comprometidos con la excelencia y el avance tecnológico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bf3f3f-ff59-4a5a-a965-605dd27c6a45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
